{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "latestwork.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samarth0174/research_work_pointcloud/blob/master/latestwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqq3eJ93VavJ",
        "colab_type": "code",
        "outputId": "a046efc4-7c60-47af-e5a9-e246ce029129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "root_path = 'gdrive/My Drive/data1/'\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_yKOdXNXndK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htzjS3Y-u3Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Z1=[]\n",
        "P1=[]\n",
        "\n",
        "for q in range(100,131):\n",
        "  filname = root_path + str(q) + \".mat\"\n",
        "  #import\n",
        "  mat = scipy.io.loadmat(filname)['Ftr']\n",
        "  L1=list(mat)\n",
        "\n",
        "  # LRFs\n",
        "  for i in range(0,500):\n",
        "    Z=[]\n",
        "    for j in range(0,2):\n",
        "      Z.append(L1[0][0][i][0][j])\n",
        "    K=np.ndarray.tolist(np.asarray(Z))\n",
        "    Z1.append(K)\n",
        "  # print(Z1)\n",
        "\n",
        "  # Neighbours\n",
        "  for i in range(0,500):\n",
        "    \n",
        "    K=np.ndarray.tolist(np.asarray(L1[0][1][i][0]))\n",
        "    P1.append(K)\n",
        "  # print(P1)\n",
        "\n",
        "  \n",
        "  \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTMSyoVshcHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for q in range(131,160):\n",
        "  filname = root_path + str(q) + \".mat\"\n",
        "  #import\n",
        "  mat = scipy.io.loadmat(filname)['Ftr']\n",
        "  L1=list(mat)\n",
        "\n",
        "  # LRFs\n",
        "  for i in range(0,500):\n",
        "    Z=[]\n",
        "    for j in range(0,2):\n",
        "      Z.append(L1[0][0][i][0][j])\n",
        "    K=np.ndarray.tolist(np.asarray(Z))\n",
        "    Z1.append(K)\n",
        "  # print(Z1)\n",
        "\n",
        "  # Neighbours\n",
        "  for i in range(0,500):\n",
        "    \n",
        "    K=np.ndarray.tolist(np.asarray(L1[0][1][i][0]))\n",
        "    P1.append(K)\n",
        "  # print(P1)\n",
        "\n",
        "  \n",
        "  \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr3AnzGfsslr",
        "colab_type": "code",
        "outputId": "cf396261-5238-450d-c6a0-1982898a6c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "P1=np.asarray(P1)\n",
        "Z1=np.asarray(Z1)\n",
        "\n",
        "print((Z1.shape))\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkqKpnP0c1BV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# f=root_path + \"1.mat\"\n",
        "# mat = scipy.io.loadmat(f)['Ftr']\n",
        "# L1=list(mat)\n",
        "# Z=np.array(L1[0][1][0][0])\n",
        "# #K=np.ndarray.tolist(Z)\n",
        "# #P1.append(Z)\n",
        "# P1=np.array(Z)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u40_JiMgc1V6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# P3=np.mean(P1,axis=0)\n",
        "# print(P3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL1ukb9cu0hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(P1,Z1,test_size=0.2,random_state=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU1hV24cOUJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You will learn how to build very deep convolutional networks, using Residual Networks (ResNets)\n",
        "# In theory, very deep networks can represent very complex functions; but in practice, they are hard to train. Residual Networks, introduced by He et al., allow you to train much deeper networks than were previously practically feasible.\n",
        "\n",
        "# Let's import packages\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Dropout ,Activation, ZeroPadding1D, BatchNormalization, Flatten, Conv1D, AveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "#from resnets_utils import *\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)\n",
        "\n",
        "# Identity block\n",
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block as defined in Figure 3\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. You'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv1D(filters = F1, kernel_size = 1, strides = 1, padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    \n",
        "    # Second component of main path \n",
        "    X = Conv1D(filters = F2, kernel_size=(f), strides = (1), padding='same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path \n",
        "    X = Conv1D(filters = F3, kernel_size=(1), strides = 1, padding=\"valid\", name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name=bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation \n",
        "    X = Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7XY02lGaFh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# import os\n",
        "# import tensorflow as tf\n",
        "# from keras import optimizers\n",
        "# from keras.layers import Input\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Dense, Flatten, Reshape, Dropout\n",
        "# from keras.layers import Convolution1D, MaxPooling1D, BatchNormalization\n",
        "# from keras.layers import Lambda\n",
        "# from keras.utils import np_utils\n",
        "# import h5py\n",
        "\n",
        "\n",
        "# def mat_mul(A, B):\n",
        "#     return tf.matmul(A, B)\n",
        "\n",
        "\n",
        "# def load_h5(h5_filename):\n",
        "#     f = h5py.File(h5_filename)\n",
        "#     data = f['data'][:]\n",
        "#     label = f['label'][:]\n",
        "#     return (data, label)\n",
        "\n",
        "\n",
        "# def rotate_point_cloud(batch_data):\n",
        "#     \"\"\" Randomly rotate the point clouds to augument the dataset\n",
        "#         rotation is per shape based along up direction\n",
        "#         Input:\n",
        "#           BxNx3 array, original batch of point clouds\n",
        "#         Return:\n",
        "#           BxNx3 array, rotated batch of point clouds\n",
        "#     \"\"\"\n",
        "#     rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
        "#     for k in range(batch_data.shape[0]):\n",
        "#         rotation_angle = np.random.uniform() * 2 * np.pi\n",
        "#         cosval = np.cos(rotation_angle)\n",
        "#         sinval = np.sin(rotation_angle)\n",
        "#         rotation_matrix = np.array([[cosval, 0, sinval],\n",
        "#                                     [0, 1, 0],\n",
        "#                                     [-sinval, 0, cosval]])\n",
        "#         shape_pc = batch_data[k, ...]\n",
        "#         rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
        "#     return rotated_data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xERh5kXq5xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePLuM6yqq51B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block as defined in Figure 4\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    s -- Integer, specifying the stride to be used\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path \n",
        "    X = Conv1D(filters = F1, kernel_size= (1), strides = s,padding=\"valid\", name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "\n",
        "    # Second component of main path \n",
        "    X = Conv1D(filters = F2, kernel_size=(f), strides=(1), name = conv_name_base + '2b', padding=\"same\",kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name= bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path \n",
        "    X = Conv1D(filters = F3, kernel_size=(1), strides = (1), name= conv_name_base + '2c',padding=\"valid\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name=bn_name_base + '2c')(X)\n",
        "\n",
        "    ##### SHORTCUT PATH #### \n",
        "    X_shortcut = Conv1D(filters = F3, kernel_size= (1), strides=(s), name=conv_name_base + '1', padding=\"valid\", kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization( name=bn_name_base+'1')(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation \n",
        "    X = Add()([X_shortcut,X])\n",
        "    X = Activation(\"relu\")(X)\n",
        "    \n",
        "    \n",
        "    return X\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvDlVAjDq5vs",
        "colab_type": "code",
        "outputId": "5a43e226-d599-489f-e83f-689f35243ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#def ResNet50():\n",
        "    \"\"\"\n",
        "    Implementation of the popular ResNet50 the following architecture:\n",
        "    CONV1D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "X_input=Input(shape=(1499,3))\n",
        "\n",
        "    \n",
        "    # Zero-Padding\n",
        "X = ZeroPadding1D((3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "X = Conv1D(64, (7), strides = (2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "X = BatchNormalization(name = 'bn_conv1')(X)\n",
        "X = Activation('relu')(X)\n",
        "X = MaxPooling1D((4), strides=(4))(X)\n",
        "\n",
        "    # Stage 2\n",
        "X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s=1)\n",
        "X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "\n",
        "    # Stage 3\n",
        "X = convolutional_block(X, f=3, filters = [128,128,512], stage = 3, block='a', s=2)\n",
        "X = identity_block(X, 3, filters = [128,128,512],stage=3, block='b')\n",
        "X = identity_block(X, 3, filters = [128,128,512], stage=3, block='c')\n",
        "X = identity_block(X, 3, filters = [128,128,512], stage =3, block='d')\n",
        "X = AveragePooling1D((4), name='avg_pool0')(X)\n",
        "    \n",
        "\n",
        "\n",
        "    # Stage 4 \n",
        "X = convolutional_block(X, f=3, filters = [256,256,1024],stage=4, block='a', s=2)\n",
        "X = identity_block(X, 3, filters = [256,256,1024], stage=4, block='b')\n",
        "X = identity_block(X, 3, filters = [256, 256, 1024], stage=4, block='c')\n",
        "X = identity_block(X, 3, filters= [256,256,1024], stage=4, block='d')\n",
        "X = identity_block(X, 3, filters=[256,256,1024], stage=4, block='e')\n",
        "X = identity_block(X, 3, filters=[256,256,1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5 \n",
        "X = convolutional_block(X, f=3, filters=[256,256,2048], stage=5,block='a', s=3)\n",
        "X = identity_block(X, 3, filters=[256,256,2048], stage=5, block='b')\n",
        "X = identity_block(X,3, filters=[256,256,2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL (â‰ˆ1 line). Use \"X = AveragePooling1D(...)(X)\"\n",
        "X = AveragePooling1D((2), name='avg_pool')(X)\n",
        "    \n",
        "#X=Dropout(0.38)(X)\n",
        "    # output layer\n",
        "   # X = Flatten()(X)\n",
        "X = Dense(3, activation='linear', name='fc' + str(3), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Define the input as a tensor with shape input_shape\n",
        "   \n",
        "   # return model"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N729VbtEQCFM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11545
        },
        "outputId": "1bb85556-62d6-47db-dd63-614e387f9d36"
      },
      "source": [
        "\n",
        "# Run the following code to build the model's graph. If your implementation is not correct you will know it by checking your accuracy when running model.fit(...) below.\n",
        "#model = ResNet50()\n",
        "from keras.callbacks import EarlyStopping\n",
        "model.compile(loss='cosine_proximity', optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "history = model.fit(x_train, y_train,batch_size=64,epochs=150,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_train, y_train, verbose=0)\n",
        "trainerr = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Train loss:', score[0])\n",
        "print('Train accuracy:', score[1])\n",
        "print('Test loss:',trainerr[0])\n",
        "print('Test accuracy:',trainerr[1])\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1499, 3)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_1 (ZeroPadding1D (None, 1505, 3)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv1D)                  (None, 750, 64)      1408        zero_padding1d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 750, 64)      256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 750, 64)      0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 187, 64)      0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2a (Conv1D)         (None, 187, 64)      4160        max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 187, 64)      256         res2a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 187, 64)      0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2b (Conv1D)         (None, 187, 64)      12352       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 187, 64)      256         res2a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 187, 64)      0           bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch1 (Conv1D)          (None, 187, 256)     16640       max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2c (Conv1D)         (None, 187, 256)     16640       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 187, 256)     1024        res2a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2c (BatchNormalizati (None, 187, 256)     1024        res2a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 187, 256)     0           bn2a_branch1[0][0]               \n",
            "                                                                 bn2a_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 187, 256)     0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2a (Conv1D)         (None, 187, 64)      16448       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 187, 64)      256         res2b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 187, 64)      0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2b (Conv1D)         (None, 187, 64)      12352       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 187, 64)      256         res2b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 187, 64)      0           bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2c (Conv1D)         (None, 187, 256)     16640       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2c (BatchNormalizati (None, 187, 256)     1024        res2b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 187, 256)     0           bn2b_branch2c[0][0]              \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 187, 256)     0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2a (Conv1D)         (None, 187, 64)      16448       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2a (BatchNormalizati (None, 187, 64)      256         res2c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 187, 64)      0           bn2c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2b (Conv1D)         (None, 187, 64)      12352       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2b (BatchNormalizati (None, 187, 64)      256         res2c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 187, 64)      0           bn2c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2c (Conv1D)         (None, 187, 256)     16640       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2c (BatchNormalizati (None, 187, 256)     1024        res2c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 187, 256)     0           bn2c_branch2c[0][0]              \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 187, 256)     0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2a (Conv1D)         (None, 94, 128)      32896       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 94, 128)      512         res3a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 94, 128)      0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2b (Conv1D)         (None, 94, 128)      49280       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 94, 128)      512         res3a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 94, 128)      0           bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch1 (Conv1D)          (None, 94, 512)      131584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2c (Conv1D)         (None, 94, 512)      66048       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 94, 512)      2048        res3a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2c (BatchNormalizati (None, 94, 512)      2048        res3a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 94, 512)      0           bn3a_branch1[0][0]               \n",
            "                                                                 bn3a_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 94, 512)      0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2a (Conv1D)         (None, 94, 128)      65664       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 94, 128)      512         res3b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 94, 128)      0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2b (Conv1D)         (None, 94, 128)      49280       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 94, 128)      512         res3b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 94, 128)      0           bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2c (Conv1D)         (None, 94, 512)      66048       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2c (BatchNormalizati (None, 94, 512)      2048        res3b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 94, 512)      0           bn3b_branch2c[0][0]              \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 94, 512)      0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2a (Conv1D)         (None, 94, 128)      65664       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2a (BatchNormalizati (None, 94, 128)      512         res3c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 94, 128)      0           bn3c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2b (Conv1D)         (None, 94, 128)      49280       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2b (BatchNormalizati (None, 94, 128)      512         res3c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 94, 128)      0           bn3c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2c (Conv1D)         (None, 94, 512)      66048       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2c (BatchNormalizati (None, 94, 512)      2048        res3c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 94, 512)      0           bn3c_branch2c[0][0]              \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 94, 512)      0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2a (Conv1D)         (None, 94, 128)      65664       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2a (BatchNormalizati (None, 94, 128)      512         res3d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 94, 128)      0           bn3d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2b (Conv1D)         (None, 94, 128)      49280       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2b (BatchNormalizati (None, 94, 128)      512         res3d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 94, 128)      0           bn3d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2c (Conv1D)         (None, 94, 512)      66048       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2c (BatchNormalizati (None, 94, 512)      2048        res3d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 94, 512)      0           bn3d_branch2c[0][0]              \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 94, 512)      0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool0 (AveragePooling1D)    (None, 23, 512)      0           activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2a (Conv1D)         (None, 12, 256)      131328      avg_pool0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 12, 256)      1024        res4a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 12, 256)      0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2b (Conv1D)         (None, 12, 256)      196864      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 12, 256)      1024        res4a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 12, 256)      0           bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch1 (Conv1D)          (None, 12, 1024)     525312      avg_pool0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2c (Conv1D)         (None, 12, 1024)     263168      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 12, 1024)     4096        res4a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2c (BatchNormalizati (None, 12, 1024)     4096        res4a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 12, 1024)     0           bn4a_branch1[0][0]               \n",
            "                                                                 bn4a_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 12, 1024)     0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2a (Conv1D)         (None, 12, 256)      262400      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2a (BatchNormalizati (None, 12, 256)      1024        res4b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 12, 256)      0           bn4b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2b (Conv1D)         (None, 12, 256)      196864      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2b (BatchNormalizati (None, 12, 256)      1024        res4b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 12, 256)      0           bn4b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2c (Conv1D)         (None, 12, 1024)     263168      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2c (BatchNormalizati (None, 12, 1024)     4096        res4b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 12, 1024)     0           bn4b_branch2c[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 12, 1024)     0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2a (Conv1D)         (None, 12, 256)      262400      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2a (BatchNormalizati (None, 12, 256)      1024        res4c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 12, 256)      0           bn4c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2b (Conv1D)         (None, 12, 256)      196864      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2b (BatchNormalizati (None, 12, 256)      1024        res4c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 12, 256)      0           bn4c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2c (Conv1D)         (None, 12, 1024)     263168      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2c (BatchNormalizati (None, 12, 1024)     4096        res4c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 12, 1024)     0           bn4c_branch2c[0][0]              \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 12, 1024)     0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2a (Conv1D)         (None, 12, 256)      262400      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2a (BatchNormalizati (None, 12, 256)      1024        res4d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 12, 256)      0           bn4d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2b (Conv1D)         (None, 12, 256)      196864      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2b (BatchNormalizati (None, 12, 256)      1024        res4d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 12, 256)      0           bn4d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2c (Conv1D)         (None, 12, 1024)     263168      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2c (BatchNormalizati (None, 12, 1024)     4096        res4d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 12, 1024)     0           bn4d_branch2c[0][0]              \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 12, 1024)     0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2a (Conv1D)         (None, 12, 256)      262400      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2a (BatchNormalizati (None, 12, 256)      1024        res4e_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 12, 256)      0           bn4e_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2b (Conv1D)         (None, 12, 256)      196864      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2b (BatchNormalizati (None, 12, 256)      1024        res4e_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 12, 256)      0           bn4e_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2c (Conv1D)         (None, 12, 1024)     263168      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2c (BatchNormalizati (None, 12, 1024)     4096        res4e_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 12, 1024)     0           bn4e_branch2c[0][0]              \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 12, 1024)     0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2a (Conv1D)         (None, 12, 256)      262400      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2a (BatchNormalizati (None, 12, 256)      1024        res4f_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 12, 256)      0           bn4f_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2b (Conv1D)         (None, 12, 256)      196864      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2b (BatchNormalizati (None, 12, 256)      1024        res4f_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 12, 256)      0           bn4f_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2c (Conv1D)         (None, 12, 1024)     263168      activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2c (BatchNormalizati (None, 12, 1024)     4096        res4f_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 12, 1024)     0           bn4f_branch2c[0][0]              \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 12, 1024)     0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2a (Conv1D)         (None, 4, 256)       262400      activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2a (BatchNormalizati (None, 4, 256)       1024        res5a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 256)       0           bn5a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2b (Conv1D)         (None, 4, 256)       196864      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2b (BatchNormalizati (None, 4, 256)       1024        res5a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 256)       0           bn5a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch1 (Conv1D)          (None, 4, 2048)      2099200     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2c (Conv1D)         (None, 4, 2048)      526336      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch1 (BatchNormalizatio (None, 4, 2048)      8192        res5a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2c (BatchNormalizati (None, 4, 2048)      8192        res5a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 4, 2048)      0           bn5a_branch1[0][0]               \n",
            "                                                                 bn5a_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 2048)      0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2a (Conv1D)         (None, 4, 256)       524544      activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2a (BatchNormalizati (None, 4, 256)       1024        res5b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 256)       0           bn5b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2b (Conv1D)         (None, 4, 256)       196864      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2b (BatchNormalizati (None, 4, 256)       1024        res5b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 256)       0           bn5b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2c (Conv1D)         (None, 4, 2048)      526336      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2c (BatchNormalizati (None, 4, 2048)      8192        res5b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 4, 2048)      0           bn5b_branch2c[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 2048)      0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2a (Conv1D)         (None, 4, 256)       524544      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2a (BatchNormalizati (None, 4, 256)       1024        res5c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 256)       0           bn5c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2b (Conv1D)         (None, 4, 256)       196864      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2b (BatchNormalizati (None, 4, 256)       1024        res5c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 256)       0           bn5c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2c (Conv1D)         (None, 4, 2048)      526336      activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2c (BatchNormalizati (None, 4, 2048)      8192        res5c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 4, 2048)      0           bn5c_branch2c[0][0]              \n",
            "                                                                 activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 2048)      0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (AveragePooling1D)     (None, 2, 2048)      0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "fc3 (Dense)                     (None, 2, 3)         6147        avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 11,380,227\n",
            "Trainable params: 11,330,179\n",
            "Non-trainable params: 50,048\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 24000 samples, validate on 6000 samples\n",
            "Epoch 1/150\n",
            "24000/24000 [==============================] - 70s 3ms/step - loss: -0.1466 - acc: 0.4276 - val_loss: -0.1808 - val_acc: 0.4613\n",
            "Epoch 2/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.1778 - acc: 0.4506 - val_loss: -0.2089 - val_acc: 0.4632\n",
            "Epoch 3/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.2186 - acc: 0.4647 - val_loss: -0.2507 - val_acc: 0.4833\n",
            "Epoch 4/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.2527 - acc: 0.4737 - val_loss: -0.2879 - val_acc: 0.4788\n",
            "Epoch 5/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.2764 - acc: 0.4744 - val_loss: -0.2728 - val_acc: 0.4557\n",
            "Epoch 6/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.2836 - acc: 0.4807 - val_loss: -0.3067 - val_acc: 0.4849\n",
            "Epoch 7/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.2889 - acc: 0.4797 - val_loss: -0.3024 - val_acc: 0.4943\n",
            "Epoch 8/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.2921 - acc: 0.4817 - val_loss: -0.2838 - val_acc: 0.4799\n",
            "Epoch 9/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.2942 - acc: 0.4828 - val_loss: -0.3010 - val_acc: 0.4712\n",
            "Epoch 10/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.2986 - acc: 0.4842 - val_loss: -0.2950 - val_acc: 0.4735\n",
            "Epoch 11/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.3028 - acc: 0.4883 - val_loss: -0.3137 - val_acc: 0.4985\n",
            "Epoch 12/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.3044 - acc: 0.4888 - val_loss: -0.3075 - val_acc: 0.4881\n",
            "Epoch 13/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3048 - acc: 0.4920 - val_loss: -0.3006 - val_acc: 0.4826\n",
            "Epoch 14/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3102 - acc: 0.4951 - val_loss: -0.3286 - val_acc: 0.4986\n",
            "Epoch 15/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.3151 - acc: 0.4954 - val_loss: -0.3239 - val_acc: 0.4998\n",
            "Epoch 16/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3100 - acc: 0.4961 - val_loss: -0.3137 - val_acc: 0.4963\n",
            "Epoch 17/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3126 - acc: 0.4959 - val_loss: -0.3208 - val_acc: 0.4990\n",
            "Epoch 18/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3132 - acc: 0.4974 - val_loss: -0.3358 - val_acc: 0.5125\n",
            "Epoch 19/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3213 - acc: 0.4984 - val_loss: -0.3332 - val_acc: 0.5092\n",
            "Epoch 20/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3197 - acc: 0.5012 - val_loss: -0.3253 - val_acc: 0.5134\n",
            "Epoch 21/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3249 - acc: 0.5007 - val_loss: -0.3362 - val_acc: 0.5078\n",
            "Epoch 22/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3224 - acc: 0.5047 - val_loss: -0.3315 - val_acc: 0.5025\n",
            "Epoch 23/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3270 - acc: 0.5012 - val_loss: -0.3328 - val_acc: 0.5066\n",
            "Epoch 24/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3287 - acc: 0.5023 - val_loss: -0.3312 - val_acc: 0.4993\n",
            "Epoch 25/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3259 - acc: 0.5024 - val_loss: -0.3324 - val_acc: 0.5122\n",
            "Epoch 26/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3314 - acc: 0.5051 - val_loss: -0.3483 - val_acc: 0.5196\n",
            "Epoch 27/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3350 - acc: 0.5065 - val_loss: -0.3403 - val_acc: 0.5170\n",
            "Epoch 28/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3367 - acc: 0.5057 - val_loss: -0.3487 - val_acc: 0.5122\n",
            "Epoch 29/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3433 - acc: 0.5052 - val_loss: -0.3386 - val_acc: 0.5043\n",
            "Epoch 30/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3404 - acc: 0.5069 - val_loss: -0.3481 - val_acc: 0.5173\n",
            "Epoch 31/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3397 - acc: 0.5062 - val_loss: -0.3366 - val_acc: 0.5164\n",
            "Epoch 32/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3456 - acc: 0.5100 - val_loss: -0.3524 - val_acc: 0.5156\n",
            "Epoch 33/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3441 - acc: 0.5090 - val_loss: -0.3485 - val_acc: 0.5088\n",
            "Epoch 34/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3507 - acc: 0.5126 - val_loss: -0.3550 - val_acc: 0.5178\n",
            "Epoch 35/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3482 - acc: 0.5131 - val_loss: -0.3584 - val_acc: 0.5212\n",
            "Epoch 36/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.3457 - acc: 0.5066 - val_loss: -0.3566 - val_acc: 0.5233\n",
            "Epoch 37/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3524 - acc: 0.5112 - val_loss: -0.3549 - val_acc: 0.5194\n",
            "Epoch 38/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3539 - acc: 0.5121 - val_loss: -0.3546 - val_acc: 0.5134\n",
            "Epoch 39/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.3552 - acc: 0.5109 - val_loss: -0.3454 - val_acc: 0.5112\n",
            "Epoch 40/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.3537 - acc: 0.5127 - val_loss: -0.3535 - val_acc: 0.5187\n",
            "Epoch 41/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3581 - acc: 0.5154 - val_loss: -0.3496 - val_acc: 0.5150\n",
            "Epoch 42/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3593 - acc: 0.5170 - val_loss: -0.3584 - val_acc: 0.5125\n",
            "Epoch 43/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3601 - acc: 0.5139 - val_loss: -0.3611 - val_acc: 0.5242\n",
            "Epoch 44/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3628 - acc: 0.5181 - val_loss: -0.3635 - val_acc: 0.5160\n",
            "Epoch 45/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3633 - acc: 0.5217 - val_loss: -0.3731 - val_acc: 0.5263\n",
            "Epoch 46/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3688 - acc: 0.5221 - val_loss: -0.3645 - val_acc: 0.5158\n",
            "Epoch 47/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3669 - acc: 0.5198 - val_loss: -0.3637 - val_acc: 0.5265\n",
            "Epoch 48/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3673 - acc: 0.5198 - val_loss: -0.3804 - val_acc: 0.5271\n",
            "Epoch 49/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3696 - acc: 0.5174 - val_loss: -0.3756 - val_acc: 0.5275\n",
            "Epoch 50/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3783 - acc: 0.5244 - val_loss: -0.3813 - val_acc: 0.5280\n",
            "Epoch 51/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3730 - acc: 0.5212 - val_loss: -0.3763 - val_acc: 0.5209\n",
            "Epoch 52/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3784 - acc: 0.5222 - val_loss: -0.3895 - val_acc: 0.5312\n",
            "Epoch 53/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3829 - acc: 0.5272 - val_loss: -0.3785 - val_acc: 0.5242\n",
            "Epoch 54/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3817 - acc: 0.5240 - val_loss: -0.3811 - val_acc: 0.5265\n",
            "Epoch 55/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3843 - acc: 0.5270 - val_loss: -0.3821 - val_acc: 0.5288\n",
            "Epoch 56/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3834 - acc: 0.5294 - val_loss: -0.3881 - val_acc: 0.5318\n",
            "Epoch 57/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3888 - acc: 0.5323 - val_loss: -0.3930 - val_acc: 0.5352\n",
            "Epoch 58/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3871 - acc: 0.5285 - val_loss: -0.3893 - val_acc: 0.5312\n",
            "Epoch 59/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3871 - acc: 0.5306 - val_loss: -0.3883 - val_acc: 0.5347\n",
            "Epoch 60/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.3900 - acc: 0.5302 - val_loss: -0.3869 - val_acc: 0.5282\n",
            "Epoch 61/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3910 - acc: 0.5330 - val_loss: -0.3834 - val_acc: 0.5325\n",
            "Epoch 62/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3923 - acc: 0.5304 - val_loss: -0.3934 - val_acc: 0.5293\n",
            "Epoch 63/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.3968 - acc: 0.5332 - val_loss: -0.3955 - val_acc: 0.5374\n",
            "Epoch 64/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4006 - acc: 0.5381 - val_loss: -0.3956 - val_acc: 0.5403\n",
            "Epoch 65/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4033 - acc: 0.5378 - val_loss: -0.4054 - val_acc: 0.5407\n",
            "Epoch 66/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.4016 - acc: 0.5351 - val_loss: -0.3990 - val_acc: 0.5423\n",
            "Epoch 67/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4024 - acc: 0.5401 - val_loss: -0.4003 - val_acc: 0.5439\n",
            "Epoch 68/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4049 - acc: 0.5377 - val_loss: -0.4108 - val_acc: 0.5413\n",
            "Epoch 69/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.4087 - acc: 0.5413 - val_loss: -0.4004 - val_acc: 0.5398\n",
            "Epoch 70/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4095 - acc: 0.5432 - val_loss: -0.4011 - val_acc: 0.5388\n",
            "Epoch 71/150\n",
            "24000/24000 [==============================] - 52s 2ms/step - loss: -0.4118 - acc: 0.5428 - val_loss: -0.4060 - val_acc: 0.5442\n",
            "Epoch 72/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4136 - acc: 0.5456 - val_loss: -0.4121 - val_acc: 0.5499\n",
            "Epoch 73/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4168 - acc: 0.5485 - val_loss: -0.4007 - val_acc: 0.5467\n",
            "Epoch 74/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4204 - acc: 0.5510 - val_loss: -0.4066 - val_acc: 0.5459\n",
            "Epoch 75/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4206 - acc: 0.5508 - val_loss: -0.4052 - val_acc: 0.5438\n",
            "Epoch 76/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4156 - acc: 0.5499 - val_loss: -0.4139 - val_acc: 0.5486\n",
            "Epoch 77/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4259 - acc: 0.5531 - val_loss: -0.4160 - val_acc: 0.5544\n",
            "Epoch 78/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4275 - acc: 0.5534 - val_loss: -0.4175 - val_acc: 0.5507\n",
            "Epoch 79/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4279 - acc: 0.5539 - val_loss: -0.4151 - val_acc: 0.5478\n",
            "Epoch 80/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4306 - acc: 0.5585 - val_loss: -0.4203 - val_acc: 0.5550\n",
            "Epoch 81/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4322 - acc: 0.5589 - val_loss: -0.4019 - val_acc: 0.5469\n",
            "Epoch 82/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4323 - acc: 0.5599 - val_loss: -0.4215 - val_acc: 0.5562\n",
            "Epoch 83/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4374 - acc: 0.5590 - val_loss: -0.4182 - val_acc: 0.5476\n",
            "Epoch 84/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4418 - acc: 0.5641 - val_loss: -0.4238 - val_acc: 0.5548\n",
            "Epoch 85/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4429 - acc: 0.5644 - val_loss: -0.4157 - val_acc: 0.5512\n",
            "Epoch 86/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4447 - acc: 0.5679 - val_loss: -0.4286 - val_acc: 0.5603\n",
            "Epoch 87/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4458 - acc: 0.5671 - val_loss: -0.4209 - val_acc: 0.5574\n",
            "Epoch 88/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4499 - acc: 0.5719 - val_loss: -0.4178 - val_acc: 0.5579\n",
            "Epoch 89/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4511 - acc: 0.5704 - val_loss: -0.4338 - val_acc: 0.5581\n",
            "Epoch 90/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4556 - acc: 0.5734 - val_loss: -0.4198 - val_acc: 0.5543\n",
            "Epoch 91/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4546 - acc: 0.5734 - val_loss: -0.4315 - val_acc: 0.5636\n",
            "Epoch 92/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4589 - acc: 0.5735 - val_loss: -0.4339 - val_acc: 0.5522\n",
            "Epoch 93/150\n",
            "24000/24000 [==============================] - 50s 2ms/step - loss: -0.4601 - acc: 0.5749 - val_loss: -0.4341 - val_acc: 0.5685\n",
            "Epoch 94/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4630 - acc: 0.5777 - val_loss: -0.4357 - val_acc: 0.5656\n",
            "Epoch 95/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4655 - acc: 0.5793 - val_loss: -0.4333 - val_acc: 0.5521\n",
            "Epoch 96/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4652 - acc: 0.5792 - val_loss: -0.4354 - val_acc: 0.5554\n",
            "Epoch 97/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4735 - acc: 0.5824 - val_loss: -0.4445 - val_acc: 0.5626\n",
            "Epoch 98/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4670 - acc: 0.5795 - val_loss: -0.4537 - val_acc: 0.5711\n",
            "Epoch 99/150\n",
            "24000/24000 [==============================] - 48s 2ms/step - loss: -0.4812 - acc: 0.5882 - val_loss: -0.4434 - val_acc: 0.5617\n",
            "Epoch 100/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4862 - acc: 0.5934 - val_loss: -0.4539 - val_acc: 0.5776\n",
            "Epoch 101/150\n",
            "24000/24000 [==============================] - 48s 2ms/step - loss: -0.4834 - acc: 0.5912 - val_loss: -0.4520 - val_acc: 0.5655\n",
            "Epoch 102/150\n",
            "24000/24000 [==============================] - 48s 2ms/step - loss: -0.4807 - acc: 0.5900 - val_loss: -0.4441 - val_acc: 0.5598\n",
            "Epoch 103/150\n",
            "24000/24000 [==============================] - 48s 2ms/step - loss: -0.4904 - acc: 0.5957 - val_loss: -0.4578 - val_acc: 0.5847\n",
            "Epoch 104/150\n",
            "24000/24000 [==============================] - 48s 2ms/step - loss: -0.4937 - acc: 0.5993 - val_loss: -0.4554 - val_acc: 0.5729\n",
            "Epoch 105/150\n",
            "24000/24000 [==============================] - 51s 2ms/step - loss: -0.4866 - acc: 0.5925 - val_loss: -0.4334 - val_acc: 0.5606\n",
            "Epoch 106/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4928 - acc: 0.5968 - val_loss: -0.4596 - val_acc: 0.5726\n",
            "Epoch 107/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.4991 - acc: 0.6023 - val_loss: -0.4557 - val_acc: 0.5748\n",
            "Epoch 108/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5078 - acc: 0.6056 - val_loss: -0.4701 - val_acc: 0.5867\n",
            "Epoch 109/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5100 - acc: 0.6049 - val_loss: -0.4628 - val_acc: 0.5758\n",
            "Epoch 110/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5100 - acc: 0.6087 - val_loss: -0.4675 - val_acc: 0.5756\n",
            "Epoch 111/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5203 - acc: 0.6122 - val_loss: -0.4622 - val_acc: 0.5760\n",
            "Epoch 112/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5196 - acc: 0.6127 - val_loss: -0.4807 - val_acc: 0.5892\n",
            "Epoch 113/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5236 - acc: 0.6135 - val_loss: -0.4792 - val_acc: 0.5862\n",
            "Epoch 114/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5244 - acc: 0.6170 - val_loss: -0.4864 - val_acc: 0.5908\n",
            "Epoch 115/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5287 - acc: 0.6192 - val_loss: -0.4842 - val_acc: 0.5854\n",
            "Epoch 116/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5328 - acc: 0.6219 - val_loss: -0.4856 - val_acc: 0.5896\n",
            "Epoch 117/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5313 - acc: 0.6192 - val_loss: -0.4754 - val_acc: 0.5928\n",
            "Epoch 118/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5345 - acc: 0.6267 - val_loss: -0.4773 - val_acc: 0.5882\n",
            "Epoch 119/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5418 - acc: 0.6235 - val_loss: -0.4918 - val_acc: 0.5976\n",
            "Epoch 120/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5488 - acc: 0.6324 - val_loss: -0.4920 - val_acc: 0.5964\n",
            "Epoch 121/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5526 - acc: 0.6322 - val_loss: -0.5023 - val_acc: 0.6064\n",
            "Epoch 122/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5479 - acc: 0.6323 - val_loss: -0.4968 - val_acc: 0.5927\n",
            "Epoch 123/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5546 - acc: 0.6349 - val_loss: -0.4945 - val_acc: 0.6040\n",
            "Epoch 124/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5515 - acc: 0.6312 - val_loss: -0.4991 - val_acc: 0.6084\n",
            "Epoch 125/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5625 - acc: 0.6388 - val_loss: -0.4951 - val_acc: 0.5971\n",
            "Epoch 126/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5660 - acc: 0.6432 - val_loss: -0.4973 - val_acc: 0.6035\n",
            "Epoch 127/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5673 - acc: 0.6433 - val_loss: -0.5128 - val_acc: 0.6085\n",
            "Epoch 128/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5668 - acc: 0.6445 - val_loss: -0.5040 - val_acc: 0.6011\n",
            "Epoch 129/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5702 - acc: 0.6440 - val_loss: -0.5041 - val_acc: 0.6089\n",
            "Epoch 130/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5770 - acc: 0.6458 - val_loss: -0.5031 - val_acc: 0.6035\n",
            "Epoch 131/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5759 - acc: 0.6475 - val_loss: -0.5173 - val_acc: 0.6143\n",
            "Epoch 132/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5809 - acc: 0.6491 - val_loss: -0.5084 - val_acc: 0.6097\n",
            "Epoch 133/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5826 - acc: 0.6522 - val_loss: -0.4984 - val_acc: 0.6023\n",
            "Epoch 134/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5825 - acc: 0.6529 - val_loss: -0.5095 - val_acc: 0.6141\n",
            "Epoch 135/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5915 - acc: 0.6562 - val_loss: -0.5208 - val_acc: 0.6135\n",
            "Epoch 136/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5970 - acc: 0.6608 - val_loss: -0.5128 - val_acc: 0.6146\n",
            "Epoch 137/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.6044 - acc: 0.6619 - val_loss: -0.5157 - val_acc: 0.6137\n",
            "Epoch 138/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.5946 - acc: 0.6585 - val_loss: -0.5111 - val_acc: 0.6103\n",
            "Epoch 139/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.6028 - acc: 0.6651 - val_loss: -0.5211 - val_acc: 0.6158\n",
            "Epoch 140/150\n",
            "24000/24000 [==============================] - 48s 2ms/step - loss: -0.6048 - acc: 0.6640 - val_loss: -0.5110 - val_acc: 0.6123\n",
            "Epoch 141/150\n",
            "24000/24000 [==============================] - 48s 2ms/step - loss: -0.6077 - acc: 0.6659 - val_loss: -0.5215 - val_acc: 0.6234\n",
            "Epoch 142/150\n",
            "24000/24000 [==============================] - 52s 2ms/step - loss: -0.6110 - acc: 0.6704 - val_loss: -0.5185 - val_acc: 0.6169\n",
            "Epoch 143/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.6116 - acc: 0.6684 - val_loss: -0.4934 - val_acc: 0.5947\n",
            "Epoch 144/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.6127 - acc: 0.6661 - val_loss: -0.5264 - val_acc: 0.6229\n",
            "Epoch 145/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.6226 - acc: 0.6717 - val_loss: -0.5222 - val_acc: 0.6113\n",
            "Epoch 146/150\n",
            "24000/24000 [==============================] - 48s 2ms/step - loss: -0.6233 - acc: 0.6736 - val_loss: -0.5212 - val_acc: 0.6178\n",
            "Epoch 147/150\n",
            "24000/24000 [==============================] - 48s 2ms/step - loss: -0.6258 - acc: 0.6775 - val_loss: -0.5163 - val_acc: 0.6167\n",
            "Epoch 148/150\n",
            "24000/24000 [==============================] - 52s 2ms/step - loss: -0.6289 - acc: 0.6791 - val_loss: -0.5181 - val_acc: 0.6221\n",
            "Epoch 149/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.6310 - acc: 0.6782 - val_loss: -0.5270 - val_acc: 0.6247\n",
            "Epoch 150/150\n",
            "24000/24000 [==============================] - 49s 2ms/step - loss: -0.6223 - acc: 0.6718 - val_loss: -0.5281 - val_acc: 0.6258\n",
            "Train loss: -0.6137946531375249\n",
            "Train accuracy: 0.6693125\n",
            "Test loss: -0.5084901677767436\n",
            "Test accuracy: 0.61275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XUjPMWCMceg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('normalweights_change1.h5')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fz6u8JXCXID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "47cd4318-453c-41b1-9362-6a9a33e4e96c"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch') \n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('losss')\n",
        "plt.xlabel('Epoch') \n",
        "plt.legend(['Train', 'Test'], loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOq7X0S8UAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVxkekkqLWV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jch9r3yp50Z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}